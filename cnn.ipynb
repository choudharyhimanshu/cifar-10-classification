{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def reshape_img(img):\n",
    "    img = img.reshape((3,32*32))\n",
    "    v = np.zeros((32,32,3),dtype=np.uint8)\n",
    "    for i in range(32):\n",
    "        for j in range(32):\n",
    "            v[i][j][0] = img[0][i*32+j]\n",
    "            v[i][j][1] = img[1][i*32+j]\n",
    "            v[i][j][2] = img[2][i*32+j]  \n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "DATASET_DIR = \"cifar-10-batches-py/\"\n",
    "DATASET_FILES = [\"data_batch_1\",\"data_batch_2\",\"data_batch_3\",\"data_batch_4\",\"data_batch_5\"]\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "for file in DATASET_FILES:\n",
    "    data = unpickle(DATASET_DIR + file)\n",
    "    imgs.extend(data[\"data\"])\n",
    "    labels.extend(data[\"labels\"])\n",
    "    \n",
    "dataset = pd.DataFrame({'img': imgs, 'label': labels})\n",
    "dataset[\"img\"] = dataset.img.map(reshape_img)\n",
    "dataset.to_pickle(\"dataset/train.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "DATASET_DIR = \"cifar-10-batches-py/\"\n",
    "DATASET_FILES = [\"test_batch\"]\n",
    "\n",
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "for file in DATASET_FILES:\n",
    "    data = unpickle(DATASET_DIR + file)\n",
    "    imgs.extend(data[\"data\"])\n",
    "    labels.extend(data[\"labels\"])\n",
    "    \n",
    "dataset = pd.DataFrame({'img': imgs, 'label': labels})\n",
    "dataset[\"img\"] = dataset.img.map(reshape_img)\n",
    "dataset.to_pickle(\"dataset/test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"dataset/\"\n",
    "DATASET_FILE = \"train.pkl\"\n",
    "\n",
    "LABELS = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "# LABELS = ['airplane','automobile','bird','cat','deer','frog','horse','ship','truck']\n",
    "\n",
    "dataset = pd.read_pickle(DATASET_DIR + DATASET_FILE)\n",
    "dataset = dataset.sample(3000,random_state=7)\n",
    "# dataset = dataset[dataset.label != 3]\n",
    "# dataset = dataset[dataset.label != 5]\n",
    "\n",
    "SUFFIX = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = train_test_split(dataset,test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten,Convolution2D,MaxPooling2D,ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for v in dataset_train.img.values:\n",
    "    X_train.append(np.array(v))\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "X_test = []\n",
    "for v in dataset_test.img.values:\n",
    "    X_test.append(np.array(v))\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = to_categorical(dataset_train.label.values)\n",
    "y_test = to_categorical(dataset_test.label.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "L_RATE = 0.01\n",
    "DECAY = L_RATE/EPOCHS\n",
    "\n",
    "sgd = SGD(lr=L_RATE, momentum=0.9, decay=DECAY, nesterov=False)\n",
    "\n",
    "# Create Keras model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', activation='relu',input_shape=(32,32,3)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(128, 3, 3, border_mode='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,validation_split=0.2,batch_size=64, nb_epoch=EPOCHS,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for pred in model.predict(X_test):\n",
    "    preds.append(pred.argmax())\n",
    "    \n",
    "y_test_labels = []\n",
    "for y in y_test:\n",
    "    y_test_labels.append(y.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open(\"output/score\"+str(SUFFIX)+\".txt\",'w+')\n",
    "f.write(\"Dataset size: %d\\n\"%(len(dataset)))\n",
    "f.write(\"Epochs: %d\\n\"%(EPOCHS))\n",
    "f.write(\"%s\\n\\n\" % str(score))\n",
    "f.write(classification_report(y_test_labels,preds,target_names=LABELS))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(preds,y_test_labels)\n",
    "df_cm = pd.DataFrame(cm, index = LABELS,\n",
    "                  columns = LABELS)\n",
    "sn.heatmap(df_cm,annot=True,fmt=\"\")\n",
    "plt.savefig('output/fig'+str(SUFFIX)+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame({'Predicted': preds,'Actual': y_test_labels})\n",
    "out.to_csv(\"output/preds_\"+str(SUFFIX)+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test_labels,preds,target_names=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vr_project]",
   "language": "python",
   "name": "conda-env-vr_project-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
